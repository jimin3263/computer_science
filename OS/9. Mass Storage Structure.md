# Mass Storage Structure

## 보조기억장치


### magnetic disk: 초당 60 ~ 250 / BPM(분당 -> 10000이상은 해야함)
- 원반표면: 데이터 씀
- 헤드: 데이터 읽고 씀
- positioning time
  - **seek time (직선 운동)** => track
  - **rotationale latency (회전)** => sector

<img src ="https://user-images.githubusercontent.com/50178026/122063705-77937480-ce2b-11eb-9ab2-40f249423940.png" width = "600" height = "450">

- Platter: 원반, 여러장 -> 자기 물질
- head: 자화 시킴 -> 표면 데이터 접근, 위 아래로, 미세간격 두고 떠있음/ arm의 끝에 head
- spindle: platter 연결 시킴
- track -> 앞 뒤로, 집합 -> cylinder 
- sector -> 회전, 집합 -> cluster(연속적)
- 하드 디스크 주소 => <head#, track#, sector#>
- platter의 크기 -> 클수록 저장공간 큼
- 성능
  - average access time = seek + lotation
  - average I/O time = average access time + amount to transfer/ transfer rate) + controller overhead
  - logical block = cluster : 소프트웨어가 쓰는 기본 단위
  - 모든 sector가 다 같진 않음

### SSD
  - 비휘발성
  - 반도체 (하드디스크는 기계식)
  - expensive
  - flash memory -> 수명(덮어쓰기 제한)
  - less capacity
  - 빠름 -> 속도가 빨라서 Bus 자체가 bottleneck 될 수도
  - reliable -> 오래 저장
  - seek, rotational latency는 없음

### Magnetic Tape
  - 극히 일부
  - 2차 저장 장치: 데이터 유지, 바로 접근
  - 3차 저장 장치: 매일 접근하지 않음 (은행에 사용) => 마그네틱 테이프
  - 반영구적 저장, 많은 저장
  - 카트리지 -> 풀었다가 감았다가 -> 특정 위치 이동에 시간 오래 걸림
  - 전송률은 disk와 비슷하고 seek, lotational 시간이 오래 걸림
  - 저장 용량 제한 없음

## Disk Scheduling
- head를 어떤 순서로 이동 ?
- seek time 줄임
- disk bandwidth = 전체 byte 수/ 입출력 시간
- OS(paging, swapping), System processes, Users prossesses
- input/ output mode, main memory <-> disk memory => disk address, memory address, sector의 수
- request queue에서 대기
- track == cylinder

### FCFS
- request 순서대로
- 이동 거리와 seektime는 비례 관계

### SSTF (Shortest seek time => starvation)
- 현재 head 기준으로 가장 작은 seek time
- 가장 가까운 거부터

### SCAN (= elevator algorithm)
- 한쪽 끝에서 한쪽 끝 도중에서 만남
- 중간은 대기시간 길지 않아서 대기 시간 일정하지 않음

### C-SCAN (circular -> 199 다음 0 인 것처럼)
- 일정한 대기시간
- 한쪽 끝에서 반대쪽 끝으로 이동, 한쪽 방향 처리, 반대쪽은 처리하지 않음

### C-LOOK
- 폭 조정
- 방향 전환이 큐에 따라 달라짐 -> 왕복 폭 달라짐 -> 대기시간 달라짐

### Selecting Disk Scheduling Algorithm
- SSTF: 짧은 seek time
- Scan, c-scan -> starvation 없음, 부화가 많을때
- file 저장할때 디스크 블록 할당하는 방식에 따라서 -> 디스크 전체 트랙에 흩어있으면 -> 여러번 seek
- meta data에도 영향 -> 파일 부가 정보 (생성 날짜, 이름, 주소 등) => 분리되어서 저장
- 독립적인 kernel module로 구성 -> disk에 따라서 적절하게 disk algorithm 적용 가능
- SSTF, LOOK -> 기본 알고리즘
- controller + OS 스케줄러 -> 관계 정립 고려해야 함

## RAID structure
- **redundant array of inexpensive disk**
- 디스크 많이 존재 => 병렬적으로 이용
  - 성능 향상
  - data 중복 저장 -> 신뢰성 항상
- **Mirroring**: 복사본 만듦 -> 논리적으로 하나의 디스크 -> 내부적으로는 두 개의 디스크
  - mean time to faulure: 고장나기 걸리는 시간
  - 개별 디스크의 mean time to failure(100000), mean time to repair(10) => 100,000^2 / 2*10 
- **Data striping**
  - 디스크 여러개, 저장하고자 하는 데이터를 1/n개로 나눠서 n개에 걸쳐서
  - bit-level, block- level  
*-> 시간 더 짧게 걸림, 작은 access 처리량 높이고 대규모는 응답 시간 개선*

### RAID
- RAID 1: mirroring만
- RAID 4,5,6: block interleaved parity -> less redundancy
- RAID 1+0 : Striped mirrors
- RAID 0+1 : Mirrored stripes
- 예비 디스크: 평소에 사용하지 않다가 다른 디스크가 오류 발생하면 교체함

<img src ="https://user-images.githubusercontent.com/50178026/122078069-4325b580-ce37-11eb-9ded-ad55dd42dd1d.png" width = "400" height = "500">

- RAID 0: 
  - block level에서 striping
  - 오류상황 대처 못함
  - 성능 좋음
- RAID 1: mirroring C => 복사본
  - 성능보다는 빠른 복구
  - 공간 부담 큼
- RAID 2: ECC메모리 parity bit, bit 단위 striping P => parity bit
- RAID 3: 
  - 하나의 sector 손상 => 어떤 sector 손상됐는지 알 수 있음 
  - 다른 disk sector로부터 알 수 있음
  - 나머지 비트 parity == 저장된 parity ,손상된 bit =0
  - parity 1개, 비용 저렴
- RAID 4: block-level spriping
- RAID 5: parity 분산
  - 많은 양의 데이터 저장
- RAID 6: 
  - P + Q
  - 추가의 중복 정보 저장
  - 신뢰성이 높음
  - 추가의 중복 정보 저장

- RAID 0+1:
   - striping -> mirroring
   - 디스크 하나 고장 -> 모두 접근 불가
- RAID 1+0:
  - mirroring -> striping
  - 하나 고장 -> 나머지 접근 가능

-> 복구 능력에 따라 level 
